# MongoDB â†’ Polars â†’ MinIO(Parquet) â†’ ML Training íŒŒì´í”„ë¼ì¸

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.mongo.hooks.mongo import MongoHook
import polars as pl
import numpy as np
import json
import os
from typing import List, Dict, Any
import boto3
from botocore.exceptions import ClientError

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'polars_minio_ml_pipeline',
    default_args=default_args,
    description='Polars + MinIO ê¸°ë°˜ MongoDB â†’ ML Training íŒŒì´í”„ë¼ì¸',
    schedule_interval='@daily',
    catchup=False,
    max_active_runs=1,
    tags=['polars', 'minio', 'ml', 'transformer', 'mongodb']
)

# ===== MinIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì • =====

class MinIOManager:
    """MinIO ìŠ¤í† ë¦¬ì§€ ë§¤ë‹ˆì €"""

    def __init__(self):
        self.client = boto3.client(
            's3',
            endpoint_url='http://minio:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin',
            region_name='us-east-1'
        )
        self.bucket_name = 'mlops-data'
        self._ensure_bucket_exists()

    def _ensure_bucket_exists(self):
        """ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        try:
            self.client.head_bucket(Bucket=self.bucket_name)
        except ClientError:
            self.client.create_bucket(Bucket=self.bucket_name)
            print(f"ë²„í‚· '{self.bucket_name}' ìƒì„± ì™„ë£Œ")

    def upload_parquet(self, df: pl.DataFrame, key: str, metadata: dict = None):
        """Polars DataFrameì„ Parquetìœ¼ë¡œ MinIO ì—…ë¡œë“œ"""

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file = f"/tmp/{key.replace('/', '_')}.parquet"
        df.write_parquet(temp_file, compression='snappy')

        # MinIO ì—…ë¡œë“œ
        extra_args = {}
        if metadata:
            extra_args['Metadata'] = {k: str(v) for k, v in metadata.items()}

        self.client.upload_file(temp_file, self.bucket_name, key, ExtraArgs=extra_args)

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_file)

        print(f"MinIO ì—…ë¡œë“œ ì™„ë£Œ: s3://{self.bucket_name}/{key}")
        return f"s3://{self.bucket_name}/{key}"

    def download_parquet(self, key: str) -> pl.DataFrame:
        """MinIOì—ì„œ Parquet ë‹¤ìš´ë¡œë“œ í›„ Polars DataFrame ë°˜í™˜"""

        temp_file = f"/tmp/{key.replace('/', '_')}_download.parquet"
        self.client.download_file(self.bucket_name, key, temp_file)

        df = pl.read_parquet(temp_file)
        os.remove(temp_file)

        return df

    def list_objects(self, prefix: str) -> List[str]:
        """íŠ¹ì • prefixë¡œ ì‹œì‘í•˜ëŠ” ê°ì²´ ëª©ë¡ ë°˜í™˜"""
        response = self.client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
        return [obj['Key'] for obj in response.get('Contents', [])]

# ===== 1. MongoDB ë°ì´í„° ì¶”ì¶œ (Polars ìµœì í™”) =====

def extract_mongodb_to_polars(**context):
    """MongoDBì—ì„œ ë°ì´í„° ì¶”ì¶œ í›„ Polars DataFrameìœ¼ë¡œ ë³€í™˜"""

    execution_date = context['ds']
    start_date = datetime.strptime(execution_date, '%Y-%m-%d')
    end_date = start_date + timedelta(days=1)

    print(f"ğŸ“Š ë°ì´í„° ì¶”ì¶œ ì‹œì‘: {start_date} ~ {end_date}")

    # MongoDB ì—°ê²°
    mongo_hook = MongoHook(conn_id='mongodb_default')
    collection = mongo_hook.get_collection('user_events')

    # ì¿¼ë¦¬ ì¡°ê±´
    query = {
        'timestamp': {'$gte': start_date, '$lt': end_date},
        'event_type': {'$in': ['click', 'view', 'search', 'purchase', 'like', 'share']},
        'user_id': {'$exists': True, '$ne': None}
    }

    # ë°ì´í„° ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)
    batch_size = 50000  # PolarsëŠ” ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì— ê°•í•¨
    cursor = collection.find(query).batch_size(batch_size)

    # ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘ (Polars ìµœì í™”)
    records = []
    for doc in cursor:
        # MongoDB ObjectId ì²˜ë¦¬
        doc['_id'] = str(doc['_id'])
        doc['timestamp'] = doc['timestamp'].isoformat()
        records.append(doc)

    print(f"ğŸ“¦ ì¶”ì¶œëœ ë ˆì½”ë“œ: {len(records):,}ê°œ")

    if not records:
        raise ValueError("ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # Polars DataFrame ìƒì„± (ìŠ¤í‚¤ë§ˆ ì¶”ë¡  ìë™)
    df = pl.DataFrame(records)

    print(f"ğŸ”§ Polars DataFrame ìƒì„±: {df.shape}")
    print(f"ğŸ“‹ ì»¬ëŸ¼: {df.columns}")

    # MinIOì— Raw ë°ì´í„° ì €ì¥
    minio = MinIOManager()
    raw_key = f"raw-data/year={start_date.year}/month={start_date.month:02d}/day={start_date.day:02d}/data.parquet"

    metadata = {
        'extraction_date': execution_date,
        'record_count': len(records),
        'source': 'mongodb_user_events'
    }

    minio.upload_parquet(df, raw_key, metadata)

    return {
        'raw_data_key': raw_key,
        'record_count': len(records),
        'execution_date': execution_date,
        'schema': df.schema
    }

# ===== 2. Polars ë°ì´í„° ì •ì œ ë° ë³€í™˜ =====

def clean_data_with_polars(**context):
    """Polarsë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ë°ì´í„° ì •ì œ"""

    ti = context['task_instance']
    extract_info = ti.xcom_pull(task_ids='extract_mongodb_to_polars')

    print(f"ğŸ§¹ ë°ì´í„° ì •ì œ ì‹œì‘: {extract_info['record_count']:,}ê°œ ë ˆì½”ë“œ")

    # MinIOì—ì„œ ë°ì´í„° ë¡œë“œ
    minio = MinIOManager()
    df = minio.download_parquet(extract_info['raw_data_key'])

    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {df.shape}")

    # Polars ì²´ì´ë‹ì„ í™œìš©í•œ ê³ ì„±ëŠ¥ ë°ì´í„° ì •ì œ
    cleaned_df = (
        df
        # 1. ê¸°ë³¸ í•„í„°ë§
        .filter(
            pl.col('user_id').is_not_null() &
            pl.col('event_type').is_not_null() &
            pl.col('timestamp').is_not_null()
        )
        # 2. ë°ì´í„° íƒ€ì… ë³€í™˜
        .with_columns([
            pl.col('timestamp').str.strptime(pl.Datetime, format='%Y-%m-%dT%H:%M:%S').alias('timestamp'),
            pl.col('user_id').cast(pl.Utf8),
            pl.col('event_type').cast(pl.Utf8)
        ])
        # 3. íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
        .with_columns([
            pl.col('timestamp').dt.hour().alias('hour'),
            pl.col('timestamp').dt.weekday().alias('day_of_week'),
            pl.col('timestamp').dt.date().alias('date'),
            # ì„¸ì…˜ ID ìƒì„± (ê°™ì€ ì‚¬ìš©ìì˜ 1ì‹œê°„ ë‚´ ì´ë²¤íŠ¸ëŠ” ê°™ì€ ì„¸ì…˜)
            (pl.col('user_id') + '_' +
             pl.col('timestamp').dt.strftime('%Y%m%d%H')).alias('session_id')
        ])
        # 4. ì´ìƒê°’ ì œê±°
        .filter(
            # ë„ˆë¬´ ì§§ì€ duration ì œê±°
            pl.when(pl.col('duration_seconds').is_not_null())
            .then(pl.col('duration_seconds') >= 0)
            .otherwise(True)
        )
    )

    print(f"âœ¨ ì •ì œëœ ë°ì´í„°: {cleaned_df.shape}")

    # ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸ ìˆ˜ ê³„ì‚° (Polars ê³ ì„±ëŠ¥ ì§‘ê³„)
    user_stats = (
        cleaned_df
        .group_by('user_id')
        .agg([
            pl.count().alias('event_count'),
            pl.col('event_type').n_unique().alias('unique_events'),
            pl.col('timestamp').min().alias('first_event'),
            pl.col('timestamp').max().alias('last_event')
        ])
        .with_columns([
            (pl.col('last_event') - pl.col('first_event')).dt.total_seconds().alias('session_duration_sec')
        ])
    )

    # ìµœì†Œ ì´ë²¤íŠ¸ ìˆ˜ í•„í„°ë§ (í™œì„± ì‚¬ìš©ìë§Œ)
    min_events = 5
    valid_users = user_stats.filter(pl.col('event_count') >= min_events)['user_id']

    # ìœ íš¨ ì‚¬ìš©ìë§Œ í•„í„°ë§
    final_df = cleaned_df.filter(pl.col('user_id').is_in(valid_users))

    print(f"ğŸ‘¥ ìœ íš¨ ì‚¬ìš©ì: {valid_users.len():,}ëª…")
    print(f"ğŸ“ˆ ìµœì¢… ë°ì´í„°: {final_df.shape}")

    # ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
    quality_report = {
        'original_count': extract_info['record_count'],
        'cleaned_count': final_df.height,
        'data_retention_rate': final_df.height / extract_info['record_count'],
        'unique_users': final_df['user_id'].n_unique(),
        'unique_events': final_df['event_type'].n_unique(),
        'event_distribution': final_df['event_type'].value_counts().to_dict(),
        'date_range': {
            'start': final_df['timestamp'].min().isoformat(),
            'end': final_df['timestamp'].max().isoformat()
        }
    }

    print(f"ğŸ“Š ë°ì´í„° ë³´ì¡´ìœ¨: {quality_report['data_retention_rate']:.2%}")

    # MinIOì— ì •ì œëœ ë°ì´í„° ì €ì¥
    execution_date = extract_info['execution_date']
    start_date = datetime.strptime(execution_date, '%Y-%m-%d')

    clean_key = f"clean-data/year={start_date.year}/month={start_date.month:02d}/day={start_date.day:02d}/cleaned.parquet"
    stats_key = f"clean-data/year={start_date.year}/month={start_date.month:02d}/day={start_date.day:02d}/user_stats.parquet"

    # ì •ì œëœ ë°ì´í„° ì—…ë¡œë“œ
    minio.upload_parquet(final_df, clean_key, {
        'stage': 'cleaned',
        'record_count': final_df.height,
        'unique_users': final_df['user_id'].n_unique()
    })

    # ì‚¬ìš©ì í†µê³„ ì—…ë¡œë“œ
    minio.upload_parquet(user_stats, stats_key, {
        'stage': 'user_statistics',
        'user_count': user_stats.height
    })

    return {
        'clean_data_key': clean_key,
        'user_stats_key': stats_key,
        'quality_report': quality_report,
        'execution_date': execution_date
    }

# ===== 3. Transformer ì‹œí€€ìŠ¤ ìƒì„± (Polars ìµœì í™”) =====

def create_sequences_with_polars(**context):
    """Polarsë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ì‹œí€€ìŠ¤ ìƒì„±"""

    ti = context['task_instance']
    clean_info = ti.xcom_pull(task_ids='clean_data_with_polars')

    print(f"ğŸ”— ì‹œí€€ìŠ¤ ìƒì„± ì‹œì‘")

    # MinIOì—ì„œ ì •ì œëœ ë°ì´í„° ë¡œë“œ
    minio = MinIOManager()
    df = minio.download_parquet(clean_info['clean_data_key'])

    # ì´ë²¤íŠ¸ ì–´íœ˜ì§‘ ìƒì„±
    event_vocab = {
        'click': 1, 'view': 2, 'search': 3,
        'purchase': 4, 'like': 5, 'share': 6,
        '[PAD]': 0, '[CLS]': 7, '[SEP]': 8, '[UNK]': 9
    }

    max_sequence_length = 128  # Transformer ìµœì  ê¸¸ì´

    print(f"ğŸ“š ì´ë²¤íŠ¸ ì–´íœ˜ì§‘ í¬ê¸°: {len(event_vocab)}")

    # Polarsë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ì‹œí€€ìŠ¤ ìƒì„±
    sequence_df = (
        df
        # 1. ì´ë²¤íŠ¸ í† í°í™”
        .with_columns([
            pl.col('event_type').map_elements(
                lambda x: event_vocab.get(x, event_vocab['[UNK]']),
                return_dtype=pl.Int32
            ).alias('event_token')
        ])
        # 2. ì‚¬ìš©ìë³„ ì‹œê°„ìˆœ ì •ë ¬
        .sort(['user_id', 'timestamp'])
        # 3. ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ì§‘ê³„
        .group_by('user_id')
        .agg([
            pl.col('event_token').alias('event_sequence'),
            pl.col('hour').alias('hour_sequence'),
            pl.col('day_of_week').alias('day_sequence'),
            pl.col('timestamp').alias('timestamp_sequence'),
            pl.count().alias('sequence_length')
        ])
        # 4. ì ì ˆí•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë§Œ ì„ íƒ
        .filter(
            (pl.col('sequence_length') >= 10) &  # ìµœì†Œ ê¸¸ì´
            (pl.col('sequence_length') <= max_sequence_length * 2)  # ìµœëŒ€ ê¸¸ì´
        )
    )

    print(f"ğŸ‘¤ ì‹œí€€ìŠ¤ ìƒì„±ëœ ì‚¬ìš©ì: {sequence_df.height:,}ëª…")

    # ì‹œí€€ìŠ¤ íŒ¨ë”© ë° ë¶„í•  í•¨ìˆ˜ (Polars UDF)
    def process_sequence(events: List[int], hours: List[int], days: List[int]) -> Dict:
        """ì‹œí€€ìŠ¤ ì²˜ë¦¬: íŒ¨ë”©, ë¶„í• , CLS/SEP ì¶”ê°€"""

        sequences = []

        if len(events) <= max_sequence_length - 2:  # CLS, SEP í† í° ê³µê°„
            # ì§§ì€ ì‹œí€€ìŠ¤: íŒ¨ë”©
            padded_events = [event_vocab['[CLS]']] + events + [event_vocab['[SEP]']]
            padded_hours = [0] + hours + [0]
            padded_days = [0] + days + [0]

            # íŒ¨ë”© ì¶”ê°€
            while len(padded_events) < max_sequence_length:
                padded_events.append(event_vocab['[PAD]'])
                padded_hours.append(0)
                padded_days.append(0)

            sequences.append({
                'events': padded_events,
                'hours': padded_hours,
                'days': padded_days,
                'length': len(events) + 2,
                'attention_mask': [1 if x != event_vocab['[PAD]'] else 0 for x in padded_events]
            })
        else:
            # ê¸´ ì‹œí€€ìŠ¤: ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¶„í• 
            window_size = max_sequence_length - 2
            step_size = window_size // 2

            for i in range(0, len(events) - window_size + 1, step_size):
                window_events = events[i:i + window_size]
                window_hours = hours[i:i + window_size]
                window_days = days[i:i + window_size]

                # CLS, SEP ì¶”ê°€
                full_events = [event_vocab['[CLS]']] + window_events + [event_vocab['[SEP]']]
                full_hours = [0] + window_hours + [0]
                full_days = [0] + window_days + [0]

                sequences.append({
                    'events': full_events,
                    'hours': full_hours,
                    'days': full_days,
                    'length': len(full_events),
                    'attention_mask': [1] * len(full_events)
                })

        return sequences

    # ëª¨ë“  ì‹œí€€ìŠ¤ ì²˜ë¦¬ (Pythonìœ¼ë¡œ ì²˜ë¦¬ í›„ Polarsë¡œ ë³€í™˜)
    all_sequences = []

    for row in sequence_df.iter_rows(named=True):
        user_sequences = process_sequence(
            row['event_sequence'],
            row['hour_sequence'],
            row['day_sequence']
        )

        for seq in user_sequences:
            all_sequences.append({
                'user_id': row['user_id'],
                'input_ids': seq['events'],
                'hours': seq['hours'],
                'days': seq['days'],
                'attention_mask': seq['attention_mask'],
                'length': seq['length']
            })

    print(f"ğŸ¯ ìƒì„±ëœ ì‹œí€€ìŠ¤: {len(all_sequences):,}ê°œ")

    # Polars DataFrameìœ¼ë¡œ ë³€í™˜
    sequences_df = pl.DataFrame(all_sequences)

    # í›ˆë ¨/ê²€ì¦ ë¶„í•  (Polars ìƒ˜í”Œë§ í™œìš©)
    train_df = sequences_df.sample(fraction=0.8, seed=42)
    val_df = sequences_df.filter(~pl.col('user_id').is_in(train_df['user_id']))

    print(f"ğŸ‹ï¸ í›ˆë ¨ ì„¸íŠ¸: {train_df.height:,}ê°œ")
    print(f"âœ… ê²€ì¦ ì„¸íŠ¸: {val_df.height:,}ê°œ")

    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        'vocab_size': len(event_vocab),
        'max_sequence_length': max_sequence_length,
        'event_vocab': event_vocab,
        'train_count': train_df.height,
        'val_count': val_df.height,
        'total_sequences': sequences_df.height,
        'avg_sequence_length': float(sequences_df['length'].mean()),
        'unique_users': sequences_df['user_id'].n_unique()
    }

    # MinIOì— ì €ì¥
    execution_date = clean_info['execution_date']
    start_date = datetime.strptime(execution_date, '%Y-%m-%d')

    base_path = f"ml-sequences/year={start_date.year}/month={start_date.month:02d}/day={start_date.day:02d}"
    train_key = f"{base_path}/train_sequences.parquet"
    val_key = f"{base_path}/val_sequences.parquet"
    metadata_key = f"{base_path}/metadata.json"

    # ì‹œí€€ìŠ¤ ë°ì´í„° ì—…ë¡œë“œ
    minio.upload_parquet(train_df, train_key, {
        'stage': 'ml_sequences',
        'split': 'train',
        'sequence_count': train_df.height
    })

    minio.upload_parquet(val_df, val_key, {
        'stage': 'ml_sequences',
        'split': 'validation',
        'sequence_count': val_df.height
    })

    # ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(metadata, f, indent=2)
        temp_path = f.name

    minio.client.upload_file(temp_path, minio.bucket_name, metadata_key)
    os.unlink(temp_path)

    print(f"ğŸ’¾ MinIO ì €ì¥ ì™„ë£Œ:")
    print(f"  - í›ˆë ¨: s3://{minio.bucket_name}/{train_key}")
    print(f"  - ê²€ì¦: s3://{minio.bucket_name}/{val_key}")
    print(f"  - ë©”íƒ€: s3://{minio.bucket_name}/{metadata_key}")

    return {
        'train_key': train_key,
        'val_key': val_key,
        'metadata_key': metadata_key,
        'metadata': metadata,
        'execution_date': execution_date
    }

# ===== 4. ìµœì¢… ML ë°ì´í„° ì¤€ë¹„ =====

def prepare_final_ml_data(**context):
    """ìµœì¢… ML í›ˆë ¨ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„"""

    ti = context['task_instance']
    sequence_info = ti.xcom_pull(task_ids='create_sequences_with_polars')

    print(f"ğŸ¯ ìµœì¢… ML ë°ì´í„° ì¤€ë¹„")

    minio = MinIOManager()

    # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¡œë“œ
    train_df = minio.download_parquet(sequence_info['train_key'])
    val_df = minio.download_parquet(sequence_info['val_key'])

    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
    print(f"  - í›ˆë ¨: {train_df.shape}")
    print(f"  - ê²€ì¦: {val_df.shape}")

    # HuggingFace Datasets í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    def convert_to_hf_format(df: pl.DataFrame) -> pl.DataFrame:
        """HuggingFace Datasets í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return df.select([
            pl.col('input_ids'),
            pl.col('attention_mask'),
            pl.col('hours'),
            pl.col('days'),
            pl.col('user_id'),
            pl.col('length')
        ])

    hf_train_df = convert_to_hf_format(train_df)
    hf_val_df = convert_to_hf_format(val_df)

    # ìµœì¢… í†µê³„ ê³„ì‚°
    final_stats = {
        'dataset_info': {
            'train_size': hf_train_df.height,
            'val_size': hf_val_df.height,
            'vocab_size': sequence_info['metadata']['vocab_size'],
            'max_length': sequence_info['metadata']['max_sequence_length']
        },
        'sequence_stats': {
            'avg_length_train': float(hf_train_df['length'].mean()),
            'avg_length_val': float(hf_val_df['length'].mean()),
            'min_length': int(min(hf_train_df['length'].min(), hf_val_df['length'].min())),
            'max_length': int(max(hf_train_df['length'].max(), hf_val_df['length'].max()))
        },
        'user_stats': {
            'unique_users_train': hf_train_df['user_id'].n_unique(),
            'unique_users_val': hf_val_df['user_id'].n_unique(),
            'total_unique_users': pl.concat([hf_train_df['user_id'], hf_val_df['user_id']]).n_unique()
        }
    }

    # ìµœì¢… ë°ì´í„° ì €ì¥
    execution_date = sequence_info['execution_date']
    start_date = datetime.strptime(execution_date, '%Y-%m-%d')

    final_path = f"ml-ready/year={start_date.year}/month={start_date.month:02d}/day={start_date.day:02d}"
    final_train_key = f"{final_path}/train_final.parquet"
    final_val_key = f"{final_path}/val_final.parquet"
    final_stats_key = f"{final_path}/final_stats.json"

    # ìµœì¢… ë°ì´í„° ì—…ë¡œë“œ
    minio.upload_parquet(hf_train_df, final_train_key, {
        'stage': 'ml_ready',
        'split': 'train_final',
        'ready_for_training': 'true'
    })

    minio.upload_parquet(hf_val_df, final_val_key, {
        'stage': 'ml_ready',
        'split': 'val_final',
        'ready_for_training': 'true'
    })

    # ìµœì¢… í†µê³„ ì—…ë¡œë“œ
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(final_stats, f, indent=2)
        temp_path = f.name

    minio.client.upload_file(temp_path, minio.bucket_name, final_stats_key)
    os.unlink(temp_path)

    print(f"âœ… ìµœì¢… ML ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“ˆ í›ˆë ¨ ë°ì´í„°: {final_stats['dataset_info']['train_size']:,}ê°œ")
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {final_stats['dataset_info']['val_size']:,}ê°œ")
    print(f"ğŸ‘¥ ì´ ì‚¬ìš©ì: {final_stats['user_stats']['total_unique_users']:,}ëª…")

    return {
        'final_train_key': final_train_key,
        'final_val_key': final_val_key,
        'final_stats_key': final_stats_key,
        'final_stats': final_stats,
        'ready_for_training': True,
        's3_path': f"s3://{minio.bucket_name}/{final_path}"
    }

# ===== 5. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ =====

def validate_final_data(**context):
    """ìµœì¢… ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""

    ti = context['task_instance']
    final_info = ti.xcom_pull(task_ids='prepare_final_ml_data')

    print(f"ğŸ” ìµœì¢… ë°ì´í„° í’ˆì§ˆ ê²€ì¦")

    minio = MinIOManager()

    # ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
    try:
        train_df = minio.download_parquet(final_info['final_train_key'])
        val_df = minio.download_parquet(final_info['final_val_key'])
        print("âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        raise ValueError(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ê²€ì¦ í•­ëª©ë“¤
    validations = []

    # 1. ë°ì´í„° í˜•íƒœ ê²€ì¦
    if train_df.height > 0 and val_df.height > 0:
        validations.append("âœ… ë°ì´í„° ì¡´ì¬")
    else:
        validations.append("âŒ ë¹ˆ ë°ì´í„°ì…‹")

    # 2. í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
    required_cols = ['input_ids', 'attention_mask', 'hours', 'days']
    if all(col in train_df.columns for col in required_cols):
        validations.append("âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬")
    else:
        validations.append("âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½")

    # 3. ì‹œí€€ìŠ¤ ê¸¸ì´ ê²€ì¦
    max_len = final_info['final_stats']['dataset_info']['max_length']
    if train_df['length'].max() <= max_len:
        validations.append("âœ… ì‹œí€€ìŠ¤ ê¸¸ì´ ì ì ˆ")
    else:
        validations.append("âŒ ì‹œí€€ìŠ¤ ê¸¸ì´ ì´ˆê³¼")

    # 4. ìµœì†Œ ë°ì´í„° í¬ê¸° ê²€ì¦
    min_samples = 1000
    if train_df.height >= min_samples:
        validations.append("âœ… ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„°")
    else:
        validations.append("âš ï¸ í›ˆë ¨ ë°ì´í„° ë¶€ì¡±")

    # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
    for validation in validations:
        print(validation)

    # ì‹¤íŒ¨ ì¡°ê±´ ì²´í¬
    if "âŒ" in "\n".join(validations):
        raise ValueError("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")

    # ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
    validation_report = {
        'validation_timestamp': datetime.now().isoformat(),
        'validations': validations,
        'sample_stats': {
            'train_sample_shape': train_df.shape,
            'val_sample_shape': val_df.shape,
            'input_ids_sample': train_df['input_ids'].head(1).to_list()[0][:10],
            'attention_mask_sample': train_df['attention_mask'].head(1).to_list()[0][:10]
        },
        'data_quality_score': validations.count("âœ…") / len(validations),
        's3_location': final_info['s3_path']
    }

    print(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {validation_report['data_quality_score']:.2%}")
    print(f"ğŸ¯ S3 ê²½ë¡œ: {final_info['s3_path']}")
    print("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼! í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!")

    return validation_report

# ===== DAG Task ì •ì˜ =====

# Task 1: MongoDB â†’ Polars ì¶”ì¶œ
extract_task = PythonOperator(
    task_id='extract_mongodb_to_polars',
    python_callable=extract_mongodb_to_polars,
    dag=dag
)

# Task 2: Polars ë°ì´í„° ì •ì œ
clean_task = PythonOperator(
    task_id='clean_data_with_polars',
    python_callable=clean_data_with_polars,
    dag=dag
)

# Task 3: Polars ì‹œí€€ìŠ¤ ìƒì„±
sequence_task = PythonOperator(
    task_id='create_sequences_with_polars',
    python_callable=create_sequences_with_polars,
    dag=dag
)

# Task 4: ìµœì¢… ML ë°ì´í„° ì¤€ë¹„
ml_prep_task = PythonOperator(
    task_id='prepare_final_ml_data',
    python_callable=prepare_final_ml_data,
    dag=dag
)

# Task 5: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
validation_task = PythonOperator(
    task_id='validate_final_data',
    python_callable=validate_final_data,
    dag=dag
)

# Task ì˜ì¡´ì„± ì„¤ì •
extract_task >> clean_task >> sequence_task >> ml_prep_task >> validation_task